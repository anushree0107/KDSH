Safe Predictors for Input-Output Specification
Enforcement
Abstract
This paper presents an approach for designing neural networks, along with other
machine learning models, which adhere to a collection of input-output specifica-
tions. Our method involves the construction of a constrained predictor for each set
of compatible constraints, and combining these predictors in a safe manner using a
convex combination of their predictions. We demonstrate the applicability of this
method with synthetic datasets and on an aircraft collision avoidance problem.
1 Introduction
The increasing adoption of machine learning models, such as neural networks, in safety-critical
applications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgent
need for the development of guarantees on safety and robustness.large groups, it is important to determine whether the individual inputs were made independently. If
not, factors like group psychology and the influence of persuasive arguments can skew individual
judgments, thus negating the positive effects of crowd wisdom.
This paper focuses on forecasts concerning questions spanning political, economic, and social
domains. Each forecast includes a prediction, estimating the probability of a particular event, and
a written justification that explains the reasoning behind the prediction. Forecasts with identical
predictions can have justifications of varying strength, which, in turn, affects the perceived reliability
of the predictions. For instance, a justification that simply refers to an external source without
credibility of anonymous forecasts, enabling the development of robust aggregation strategies that do
not require tracking individual forecasters.
7
