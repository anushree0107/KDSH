neural network configurations, except for the one using only predictions and justifications.
**Encoding Questions and Justifications** The neural network that only utilizes the prediction
to represent a forecast surpasses both baseline methods. Notably, integrating the question, the
justification, or both into the forecast representation yields further improvements. These results
indicate that incorporating the question and forecaster-provided justifications into the model enhances
the accuracy of question calling.
**Calling Questions Throughout Their Life** When examining the results across the four quartiles of
a question’s duration, it’s observed that while using active forecasts is beneficial across all quartiles
for both baselines and all network configurations, the neural networks surprisingly outperform the
 Reluplex has also been used to
verify adversarial robustness. While Reluplex and other similar techniques can effectively determine
if a network satisfies a given specification, they do not offer a way to guarantee that the network will
meet those specifications. Therefore, additional methods are needed to adjust networks if it is found
that they are not meeting the desired properties.
There has been an increase in techniques for designing networks with certified adversarial robustness,
but enforcing more general safety properties in neural networks is still largely unexplored. One ap-
proach to achieving provably correct neural networks is through abstraction-refinement optimization.
This approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meet
Table 4 presents the results for selected models based on question difficulty. The weighted vote
baseline demonstrates superior performance for 75
5 Qualitative Analysis
This section provides insights into the factors that make questions more difficult to forecast and
examines the characteristics of justifications associated with incorrect and correct predictions.
**Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectly
on at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and a
higher number of active forecasts per day (31.0 vs. 26.7). This suggests that the model’s errors align
with the questions that forecasters also find challenging.
