and have analyzed the traits that contribute to their success. These studies aim to identify and cultivate
superforecasters but do not incorporate the written justifications accompanying forecasts. In contrast,
the present research develops models to call questions without using any information about the
forecasters themselves. Within the field of computational linguistics, researchers have evaluated the
language used in high-quality justifications, focusing on aspects like rating, benefit, and influence.
Other researchers have developed models to predict forecaster skill using the textual justifications
from specific datasets, such as the Good Judgment Open data, and have also applied these models
to predict the accuracy of individual forecasts in other contexts, such as company earnings reports.
 Our goal is to learn a function f(X) that predicts Y based on the input features X.
Considering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functions
fa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb are
related, either explicitly or implicitly, TL and MTL can enhance the generalization of either or both
tasks. Two tasks are deemed related when their domains are similar but their label sets differ, or when
their domains are dissimilar but their label sets are identical. Consequently, noun-noun compound
interpretation using the dataset is well-suited for TL and MTL, as the training examples are identical,
54 0 0 0.20 6.50
Readability
Flesch -49.68 50.33 65.76 80.62 121.22
Dale-Chall 0.05 6.72 7.95 9.20 19.77
Table 2 presents a fundamental analysis of the 96,664 forecast justifications in the dataset. The median
length is relatively short, consisting of one sentence and 23 tokens. Justifications mention named
entities less frequently than the questions themselves. Interestingly, half of the justifications contain
at least one negation, and 25% include three or more. This suggests that forecasters sometimes base
