A key reason for utilizing multi-task learning is to enhance generalization by making use of the
domain-specific details present in the training data of related tasks. In this study, we demonstrate that
TL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations
within a dataset marked by a highly skewed distribution of relations. This dataset is particularly
well-suited for TL and MTL experimentation, as elaborated in Section 3.
Our contributions are summarized as follows:
1. Through meticulous analysis of results, we discover that TL and MTL, especially when applied
to the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a
trained by Fares et al. (2017). If a word is not found during lookup in the embedding model, we
check if the word is uppercased and attempt to find the lowercase version. For hyphenated words
not found in the embedding vocabulary, we split the word at the hyphen and average the vectors of
its parts, if they are present in the vocabulary. If the word remains unrepresented after these steps, a
designated vector for unknown words is employed.
5.1.1 Architecture and Hyperparameters
Our selection of hyperparameters is informed by multiple rounds of experimentation with the single-
task learning model, as well as the choices made by prior work. The weights of the embedding layer
We implement the state-of-the-art LightGCN to instantiate PAAC, aiming to investigate how it alleviates popularity bias. We
compare PAAC with several debiased baselines, including re-weighting-based models, decorrelation-based models, and contrastive
learning-based models.
We utilize three widely used metrics, namely Recall@K, HR@K, and NDCG@K, to evaluate the performance of Top-K recommen-
dation. Recall@K and HR@K assess the number of target items retrieved in the recommendation results, emphasizing coverage. In
contrast, NDCG@K evaluates the positions of target items in the ranking list, with a focus on their positions in the list. We use
