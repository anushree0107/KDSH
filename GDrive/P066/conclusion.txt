The application of transfer and multi-task learning in natural language processing has gained sig-
nificant traction, yet considerable ambiguity persists regarding the effectiveness of particular task
characteristics and experimental setups. This research endeavors to clarify the benefits of TL and
MTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence of
minimally contrasting experiments and conducting thorough analysis of results and prediction errors,
we demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically
enhance predictions for low-frequency relations. Overall, our TL, and particularly our MTL models,
are better at making predictions both quantitatively and qualitatively. Notably, the improvements are
 Besides differing in the NLP tasks they investigate, the aforementioned
studies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies in
that we apply TL and MTL to learn different semantic annotations of noun-noun compounds using
the same dataset. However, our experimental design is more akin to other work in that we experiment
with initializing parameters across all layers of the neural network and concurrently train a single
MTL model on two sets of relations.
3 Task Definition and Dataset
The objective of this task is to train a model to categorize the semantic relationships between pairs
of nouns in a labeled dataset, where each pair forms a noun-noun compound. The complexity of
PCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also
have the second-highest F1 score in their datasets—except for STL on PCEDT (see Tables 4 and
5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that
lower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG in
PCEDT. Based on these insights, we can’t dismiss the possibility that our models show some degree
of lexical memorization, despite manual analysis also presenting cases where models demonstrate
generalization and correct predictions in situations where lexical memorization is impossible.
8 Conclusion
