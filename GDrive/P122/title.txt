 The central problem addressed in
this work is termed "calling a question," which refers to the process of determining a final prediction
by aggregating individual forecasts. Two strategies are employed for calling questions each day
throughout their life: considering forecasts submitted on the given day ("daily") and considering the
last forecast submitted by each forecaster ("active").
Inspired by prior research on recognizing and fostering skilled forecasters, and analyzing written
justifications to assess the quality of individual or collective forecasts, this paper investigates the
automated calling of questions throughout their duration based on the forecasts available each day.
The primary contributions are empirical findings that address the following research questions:
three elements: (a) a binary flag indicating whether the forecast was submitted on the day the question
is being called or on a previous day, (b) the prediction itself (a numerical value between 0.0 and 1.0),
and (c) a representation of the justification. The representation of the justification is also obtained
using BERT, followed by a fully connected layer with 256 neurons, ReLU activation, and dropout.
The LSTM has a hidden state with a dimensionality of 256 and processes the sequence of forecasts
as its input. During the tuning process, it was discovered that providing the representation of the
question alongside each forecast is more effective than processing forecasts independently of the
question.Model All Days Q1 Q2 Q3 Q4
Using Daily Forecasts Only
Baselines
Majority V ote (predictions) 71.89 64.59 66.59 73.26 82.22
Weighted V ote (predictions) 73.79 67.79 68.71 74.16 83.61
Neural Network Variants
Predictions Only 77.96 77.62 77.93 78.23 78.61
Predictions + Question 77.61 75.44 76.77 78.05 81.56
Predictions + Justifications 80.23 77.87 78.