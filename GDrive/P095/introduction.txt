Participants were tasked with creating 3D models of 20 distinct food items from 2D images, mim-
icking scenarios where mobile devices equipped with depth-sensing cameras are used for dietary
.
recording and nutritional tracking. The challenge was segmented into three tiers of difficulty based
on the number of images provided: approximately 200 images for easy, 30 for medium, and a single
top-view image for hard. This design aimed to rigorously test the adaptability and resilience of
proposed solutions under various realistic conditions. A notable feature of this challenge was the use
of a visible checkerboard for physical referencing and the provision of depth images for each frame,
3 53.4 57.2
I3D + pyramid 53.2 56.7 58.7
I3D + LSTM 48.2 53.1 53.1
I3D + temporal conv 52.8 57.1 58.4
I3D + sub-events 55.5 61.2 61.3
Table 5 shows the average precision for each activity class. Learning temporal structure is particularly
beneficial for frame-based features (e.g., InceptionV3), which capture less temporal information
5
compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting
as shown in Table 2. We do not apply TL (or MTL) to the output layer because it is task- or
dataset-specific.
5.3 Multi-Task Learning Models
In MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,
meaning all MTL models have two objective functions and two output layers. We implement two
MTL setups: MTLE, which features a shared embedding layer but two task-specific hidden layers,
and MTLF, which has no task-specific layers aside from the output layer (i.e., both the embedding
and hidden layers are shared). We distinguish between the auxiliary and main tasks based on which
