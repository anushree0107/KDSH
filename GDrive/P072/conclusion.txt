Mbacke et al. (2023), and the proofs presented are fundamental.
1 Introduction
Diffusion models, alongside generative adversarial networks and variational autoencoders (V AEs), are among the most influential
families of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,
as well as in various other applications.
Two primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative
models (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while
simultaneously training a backward process to reverse this transformation, enabling the creation of new samples.• a standard neural network architecture gb:X→Rm,
and then defining Gb(x;θb) =hb(gb(x;θb)).
The framework proposed here does not require an entirely separate network for each b. In many
applications, it may be advantageous for the constrained predictors to share earlier layers, thus
creating a shared representation of the input space. In addition, our definition of the safe predictor is
general and is not limited to neural networks.
In Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D
with simple neural networks. These examples show that our safe predictor can enforce arbitrary
input-output specifications using convex output constraints on neural networks, and that the learned
layer for the constrained predictors, G0andG1. Training uses a sampled subset of points from
the input space. Figure 3 shows an example of applying our safe predictor to a notional regression
problem with a 2-D input and 1-D output, using two overlapping constraints. The unconstrained
network has two hidden layers of dimension 20 and ReLU activations, followed by a fully connected
layer. The constrained predictors, G00,G10,G01, and G11, share the hidden layers but also have an
additional hidden layer of size 20 with ReLU, followed by a fully connected layer. Training uses a
sampled subset of points from the input space.
