We use PyTorch for defining our networks and performing parameter optimization. We optimize both
the unconstrained and safe predictors using the asymmetric loss function to select advisories while
also accurately predicting scores. The data is split using an 80/20 train/test split with a random seed
of 0. The optimizer is ADAM with a learning rate of 0.0003 and batch size of 216, with training for
500 epochs.
Appendix A: Proof of Theorem 2.1
Proof. Letx∈Ai. Then, σi(x) = 0 , and for all b∈Owhere bi= 0,wb(x) = 0 . Thus,
F(x) =X
 Furthermore, we introduce a modified
version of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem’s
specific parameters.
1 Introduction
The recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,
have generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,
adversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed
that generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization
component and nonconcave in the maximization component remains limited, with some research even suggesting intractability in
certain cases.
also proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step
size we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps
per descent step, achieving the same O(1/k)rate as EG.
Minty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.
It was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing
the resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a
specific rate.