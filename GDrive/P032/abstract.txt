research suggests that gains in noun-noun compound interpretation using word embeddings and
similar neural classification models might be due to lexical memorization. In other words, the models
learn that specific nouns are strong indicators of specific relations. To assess the role of lexical
memorization in our models, we quantify the number of unseen compounds that the STL, TL, and
MTL models predict correctly.
We differentiate between ’partly’ and ’completely’ unseen compounds. A compound is ’partly’
unseen if one of its constituents (left or right) is not present in the training data. A ’completely’
unseen compound is one where neither the left nor the right constituent appears in the training data.
their predictions on events that might not occur or have not yet occurred. The sentiment polarity of
3
the justifications is generally neutral. In terms of readability, both the Flesch and Dale-Chall scores
suggest that approximately a quarter of the justifications require a college-level education for full
comprehension.
Regarding verbs and nouns, an analysis using WordNet lexical files reveals that the most common
verb classes are "change" (e.g., "happen," "remain," "increase"), "social" (e.g., "vote," "support,"
"help"), "cognition" (e.g., "think," "believe," "know"), and "motion" (e.g. For instance, the temporal and locative relations in
NomBank (ARGM-TMP and ARGM-LOC, respectively) and their PCEDT counterparts (TWHEN
and LOC) exhibit relatively consistent behavior across frameworks, as they annotate many of the
same compounds. However, some relations that are theoretically similar do not align well in practice.
For example, the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank
express a somewhat related semantic concept (purpose), but there is minimal overlap between the
sets of compounds they annotate. Nevertheless, it is reasonable to assume that the semantic similarity
