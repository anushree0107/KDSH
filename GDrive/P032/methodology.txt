The dataset for the MetaFood Challenge features 20 carefully chosen food items from the MetaFood3D
dataset, each scanned in 3D and accompanied by video recordings. To ensure precise size accuracy
in the reconstructed 3D models, each food item was captured alongside a checkerboard and pattern
mat, serving as physical scaling references. The challenge is divided into three levels of difficulty,
determined by the quantity of 2D images provided for reconstruction:
• Easy: Around 200 images taken from video.
• Medium: 30 images.
• Hard: A single image from a top-down perspective.
Table 1 details the food items included in the dataset.
Table 1: MetaFood Challenge Data Details
applicability.
•Complex backgrounds and objects: The method has not been tested in environments with
complex backgrounds or highly intricate food objects.
•Capturing complexities: The method has not been evaluated under different capturing
complexities, such as varying distances and camera speeds.
•Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.
They aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve
efficiency.
6
Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance
L Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m
1 40.06 38.relatively straightforward due to the distinct differences between pitch and non-pitch frames. The
results, detailed in Table 2, reveal minimal variation across different features or models.
Table 2: Performance on segmented videos for binary pitch/non-pitch classification.
Model RGB Flow Two-stream
InceptionV3 97.46 98.44 98.67
InceptionV3 + sub-events 98.67 98.73 99.36
I3D 98.64 98.88 98.70
I3D + sub-events 98.42 98.35 98.65
6.2.1 Multi-label Classification
