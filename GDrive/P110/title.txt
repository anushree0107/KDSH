Similarly, researchers have examined how language provides insights into interpersonal interactions
and relationships. In terms of language form and function, prior research has investigated politeness,
empathy, advice, condolences, usefulness, and deception. Related to the current study’s focus,
researchers have examined the influence of Wikipedia editors and studied influence levels within
online communities. Persuasion has also been analyzed from a computational perspective, including
within the context of dialogue systems. The work presented here complements these previous studies.
The goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts,
without explicitly targeting any of the aforementioned characteristics.
Within the field of computational linguistics, the task most closely related to this research is argumen-
tation.is a 1D-convolutional layer with a kernel size {1, k}and a stride
of 1,WK∈Rd×d, WQ∈Rd×d, WV∈Rd×dare weights for keys, queries, and values of the self-attention layer, and dis the
embedding dimension. Note that all weights for GRN are shared across each time step t.
4
4.3 Multihead Dual Convolutional Self-Attention
Our approach employs a self-attention mechanism to capture global dependencies across time steps. It is embedded as part of the
DCSA architecture.to utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However, in LSTM layers, the local
context is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time
might have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and
self-attention layers, which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1∈RN×d
and a secondary input ˆx2∈RN×dand yields:
