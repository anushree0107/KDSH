observed on the ’most challenging’ inputs that include at least one constituent that was not present in
the training data. However, clear indications of ’lexical memorization’ effects are evident in our error
analysis of unseen compounds.
Typically, the transfer of representations or sharing between tasks is more effective at the embedding
layers, which represent the model’s internal representation of the compound constituents. Furthermore,
in multi-task learning, the complete sharing of model architecture across tasks degrades its capacity
to generalize when it comes to less frequent relations.
The dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound
interpretation because it links this sub-problem with broad-coverage semantic role labeling or
on noun-noun compound interpretation through the application of transfer and multi-task learning.
The application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significant
attention in recent years, yielding varying outcomes based on the specific tasks, model architectures,
and datasets involved. These varying results, combined with the fact that neither TL nor MTL has
been previously applied to noun-noun compound interpretation, motivate our thorough empirical
investigation into the use of TL and MTL for this task. Our aim is not only to add to the existing
research on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertain
their specific advantages for compound interpretation.
 Our goal is to learn a function f(X) that predicts Y based on the input features X.
Considering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functions
fa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb are
related, either explicitly or implicitly, TL and MTL can enhance the generalization of either or both
tasks. Two tasks are deemed related when their domains are similar but their label sets differ, or when
their domains are dissimilar but their label sets are identical. Consequently, noun-noun compound
interpretation using the dataset is well-suited for TL and MTL, as the training examples are identical,
