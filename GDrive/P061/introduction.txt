A key reason for utilizing multi-task learning is to enhance generalization by making use of the
domain-specific details present in the training data of related tasks. In this study, we demonstrate that
TL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations
within a dataset marked by a highly skewed distribution of relations. This dataset is particularly
well-suited for TL and MTL experimentation, as elaborated in Section 3.
Our contributions are summarized as follows:
1. Through meticulous analysis of results, we discover that TL and MTL, especially when applied
to the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a
an activity, eliminating the need for the model to identify the start and end of activities. Our methods
are based on a CNN that generates a per-frame or per-segment representation, derived from standard
two-stream CNNs using deep CNNs like I3D or InceptionV3.
Given video features vof dimensions T×D, where Trepresents the video’s temporal length and D
is the feature’s dimensionality, the usual approach for feature pooling involves max- or mean-pooling
across the temporal dimension, followed by a fully-connected layer for video clip classification, as
depicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,
losing temporal information.determines a recommender system’s effectiveness by precisely capturing the interplay between user interests and item features.
Recent studies emphasize two fundamental principles in representation learning: alignment and uniformity. The alignment principle
ensures that embeddings of similar or related items (or users) are closely clustered together, improving the system’s ability to
recommend items that align with a user’s interests. This principle is crucial when accurately reflecting user preferences through
corresponding item characteristics. Conversely, the uniformity principle ensures a balanced distribution of all embeddings across the
representation space. This approach prevents the over-concentration of embeddings in specific areas, enhancing recommendation
diversity and improving generalization to unseen data.
