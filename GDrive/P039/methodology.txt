 The central problem addressed in
this work is termed "calling a question," which refers to the process of determining a final prediction
by aggregating individual forecasts. Two strategies are employed for calling questions each day
throughout their life: considering forecasts submitted on the given day ("daily") and considering the
last forecast submitted by each forecaster ("active").
Inspired by prior research on recognizing and fostering skilled forecasters, and analyzing written
justifications to assess the quality of individual or collective forecasts, this paper investigates the
automated calling of questions throughout their duration based on the forecasts available each day.
The primary contributions are empirical findings that address the following research questions:
provided by a single expert. A classic example of this concept is the observation that the median
estimate of an ox’s weight from a large group of fair attendees was remarkably close to the actual
weight. While generally supported, the idea is not without its limitations. Historical examples
demonstrate instances where crowds behaved irrationally, and even a world chess champion was able
to defeat the combined moves of a crowd.
In the current era, the advantages of collective intelligence are widely utilized. For example, Wikipedia
relies on the contributions of volunteers, and community-driven question-answering platforms have
garnered significant attention from the research community. When compiling information from
relation ARG1, which is the most common in NomBank. Regarding the unseen right constituents,
MTLE’s 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. A
similar pattern arises when examining TLE model improvements, where most gains come from better
predictions of ARG1 and ARG0 relations.
A large portion of unseen compounds, whether partly or entirely unseen, that were misclassified by
every model, were not of type ARG1 in NomBank, or RSTR in PCEDT. This pattern, along with
correctly predicted unseen compounds primarily annotated with the most common relations, suggests
that classification models rely on lexical memorization to learn the compound relation interpretation.
