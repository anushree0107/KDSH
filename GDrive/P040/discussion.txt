would also increase the generalizability of the results to the wider population. Future work in this matter could also include the
construction of a semi-synthetic dataset based on collected data to facilitate a parallel and large-scale evaluation.
This smart home’s layout and parameters remain constant for all the participants, and we acknowledge that the transfer of this deep
learning model to other varied home settings may introduce variations in localization accuracy. For future ecological validation and
based on our current results, we anticipate the need for pre-training (e.g., a brief walkaround which is labeled) for each home, and
also suggest that some small amount of ground-truth data will need to be collected (e.g., researcher prompting of study participants to
• a standard neural network architecture gb:X→Rm,
and then defining Gb(x;θb) =hb(gb(x;θb)).
The framework proposed here does not require an entirely separate network for each b. In many
applications, it may be advantageous for the constrained predictors to share earlier layers, thus
creating a shared representation of the input space. In addition, our definition of the safe predictor is
general and is not limited to neural networks.
In Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D
with simple neural networks. These examples show that our safe predictor can enforce arbitrary
input-output specifications using convex output constraints on neural networks, and that the learned
We define our networks and perform parameter optimization using PyTorch. We optimize the
parameters of both the unconstrained network and our safe predictor using the asymmetric loss
function, guiding the network to select optimal advisories while accurately predicting scores from
the look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0. The
optimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of training
epochs is 500.
6
