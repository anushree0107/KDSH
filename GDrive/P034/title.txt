DCSA (ˆx1,ˆx2) =GRN (Norm (φ(ˆx1) + ˆx1), Norm (φ(ˆx2) + ˆx2)) (2)
with
φ(ˆx) =SA(Φk(ˆx)WQ,Φk(ˆx)WK,Φk(ˆx)WV) (3)
where GRN (.)is the Gated Residual Network to integrate dual inputs into one integrated embedding, Norm (.)is a standard layer
normalization, SA(.)is a scaled dot-product self-attention, Φk(.5)1
δfort = 0,1, . . . , T −1
The filters are then generated as:
Fm[i, t] =1
Zmexp
−(t−μi,m)2
2σ2m
i∈ {0,1, . . . , N −1}, t∈ {0,1, . . . , T −1}
where Zmis a normalization constant.
We apply these filters Fto the T×Dvideo representation through matrix multiplication, yielding an
N×Drepresentation that serves as input to a fully-connected layer for classification. This method
is shown in Fig 5(d).
We apply two different layers to produce two different outputs during training. The room-level predictions are produced via a single
conditional random field (CRF) layer in combination with a linear layer applied to the output of Eq. 7 to produce the final predictions
as:
ˆyt=CRF (φ(ht)) (7)
q′(ht) =Wpht+bp (8)
where Wp∈Rd×mandbp∈Rmare the weight and bias to learn, mis the number of room locations, and h= [h1, ..., h T]∈RT×d
is the refined embedding produced by Eq. 7. Even though the transformer can take into account neighbor information before
