Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state
as input to a fully-connected layer for classification. We frame our tasks as multi-label classification
and train these models to minimize binary cross-entropy:
L(v) =X
czclog(p(c|G(v))) + (1 −zc) log(1 −p(c|G(v)))
where G(v)is the function that pools the temporal information, and zcis the ground truth label for
class c.
5 Activity Detection in Continuous Videos
Detecting activities in continuous videos poses a greater challenge. The goal here is to classify each
frame according to the activities occurring. Unlike segmented videos, continuous videos feature
 We also experiment with combining the super- and sub-event representations
to form a three-level hierarchy for event representation.
6 Experiments
6.1 Implementation Details
For our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics
datasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable
feature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet
and Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth
compared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow
our attention on methods that frame the interpretation problem as a classification task involving a
fixed, predetermined set of relations. Various machine learning models have been applied to this
task, including nearest neighbor classifiers that use semantic similarity based on lexical resources,
kernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy
models that incorporate a wide range of lexical and surface form features, and neural networks that
rely on word embeddings or combine word embeddings with path embeddings. Among these studies,
some have utilized the same dataset. To our knowledge, TL and MTL have not been previously
applied to compound interpretation. Therefore, we review prior research on TL and MTL in other
NLP tasks.
