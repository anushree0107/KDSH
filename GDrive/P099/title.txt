Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state
as input to a fully-connected layer for classification. We frame our tasks as multi-label classification
and train these models to minimize binary cross-entropy:
L(v) =X
czclog(p(c|G(v))) + (1 −zc) log(1 −p(c|G(v)))
where G(v)is the function that pools the temporal information, and zcis the ground truth label for
class c.
5 Activity Detection in Continuous Videos
Detecting activities in continuous videos poses a greater challenge. The goal here is to classify each
frame according to the activities occurring. Unlike segmented videos, continuous videos feature
 LSTMs performed worse than the
baseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the
easiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult
(12%).
6.3 Continuous Video Activity Detection
We evaluate models extended for continuous videos using per-frame mean average precision (mAP),
with results shown in Table 8. This setting is more challenging than segmented videos, requiring
the model to identify activity start and end times and handle ambiguous negative examples. All
models improve upon the baseline per-frame classification, confirming the importance of temporal
information.to utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However, in LSTM layers, the local
context is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time
might have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and
self-attention layers, which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1∈RN×d
and a secondary input ˆx2∈RN×dand yields:
