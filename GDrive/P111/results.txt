applicability.
•Complex backgrounds and objects: The method has not been tested in environments with
complex backgrounds or highly intricate food objects.
•Capturing complexities: The method has not been evaluated under different capturing
complexities, such as varying distances and camera speeds.
•Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.
They aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve
efficiency.
6
Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance
L Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m
1 40.06 38.Table 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce
generalization error in NomBank across all scenarios, with the exception of TLH and TLEH for
completely unseen compounds, where error increases. The greatest error reductions are achieved
by MTL models across all three types of unseen compounds. Specifically, MTLE reduces the error
by approximately six points for compounds with unseen right constituents and by eleven points for
fully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituent
is unseen. It’s important to interpret these results in conjunction with the Count row in Table 9 for
a comprehensive view.To reduce popularity bias in collaborative filtering tasks, we employ a multi-task training strategy to jointly optimize the classic
recommendation loss ( LREC ), supervised alignment loss ( LSA), and re-weighting contrast loss ( LCL).
L=LREC+λ1LSA+λ2LCL+λ3||Θ||2, (7)
where Θis the set of model parameters in LREC as we do not introduce additional parameters, λ1andλ2are hyperparameters that
control the strengths of the popularity-aware supervised alignment loss and the re-weighting contrastive learning loss respectively,
andλ3is the L2regularization coefficient. After completing the model training process, we use the dot product to predict unknown
