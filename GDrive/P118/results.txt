 Besides differing in the NLP tasks they investigate, the aforementioned
studies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies in
that we apply TL and MTL to learn different semantic annotations of noun-noun compounds using
the same dataset. However, our experimental design is more akin to other work in that we experiment
with initializing parameters across all layers of the neural network and concurrently train a single
MTL model on two sets of relations.
3 Task Definition and Dataset
The objective of this task is to train a model to categorize the semantic relationships between pairs
of nouns in a labeled dataset, where each pair forms a noun-noun compound. The complexity of
PCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also
have the second-highest F1 score in their datasets—except for STL on PCEDT (see Tables 4 and
5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that
lower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG in
PCEDT. Based on these insights, we can’t dismiss the possibility that our models show some degree
of lexical memorization, despite manual analysis also presenting cases where models demonstrate
generalization and correct predictions in situations where lexical memorization is impossible.
8 Conclusion
 However, since
these improvements are relatively minor, we further analyze the results to understand if and how TL
and MTL are beneficial.
7 Results Analysis
This section provides a detailed analysis of the models’ performance, drawing on insights from the
dataset and the classification errors made by the models. The discussion in the following sections is
primarily based on the results from the test split, as it is larger than the development split.
7.1 Relation Distribution
To illustrate the complexity of the task, we depict the distribution of the most frequent relations in
NomBank and PCEDT across the three data splits in Figure 1. Notably, approximately 71.18% of the
