improve its F1 scores for less common yet more challenging semantic relations.
1 Introduction
Noun-noun compound interpretation involves determining the semantic connection between two
nouns (or noun phrases in multi-word compounds). For instance, in the compound "street protest,"
the task is to identify the semantic relationship between "street" and "protest," which is a locative
relation in this example. Given the prevalence of noun-noun compounds in natural language and its
significance to other natural language processing (NLP) tasks like question answering and information
retrieval, understanding noun-noun compounds has been extensively studied in theoretical linguistics,
psycholinguistics, and computational linguistics.
To better comprehend lexical memorization’s impact, we present the ratio of relation-specific con-
stituents in both NomBank and PCEDT, as depicted in Figure 2. We define a relation-specific
constituent as a left or right constituent that appears with only one specific relation within the training
data. Its ratio is calculated as its proportion in the full set of left or right constituents for each
8
relation. Analyzing Figure 2 reveals that NomBank relations possess higher ratios of relation-specific
constituents compared to PCEDT. This potentially makes learning the former easier if the model
solely relies on lexical memorization. Additionally, ARGM-TMP in NomBank and TWHEN in
research suggests that gains in noun-noun compound interpretation using word embeddings and
similar neural classification models might be due to lexical memorization. In other words, the models
learn that specific nouns are strong indicators of specific relations. To assess the role of lexical
memorization in our models, we quantify the number of unseen compounds that the STL, TL, and
MTL models predict correctly.
We differentiate between ’partly’ and ’completely’ unseen compounds. A compound is ’partly’
unseen if one of its constituents (left or right) is not present in the training data. A ’completely’
unseen compound is one where neither the left nor the right constituent appears in the training data.
