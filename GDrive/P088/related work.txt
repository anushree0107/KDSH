 A Rectified Linear Unit (ReLU) function φ:R→Ris given by φ(x) := max( x,0).
Given a weight vector w∈Rd, a bias b∈R, and a sign κ∈ {± 1}, a ReLU neuron is a function
φ(w, b, κ ) :Rd→Rdefined as
φ(w, b, κ )(x) =κφ(wTx+b).
Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented
by a graph with nneurons in a single hidden layer. When using the ReLU activation function, we can
apply a symmetry procedure to represent these as sums:
 While these traditional complexity
notions have been successful in classification problems, they do not apply to generic regression
problems with unbounded risk functions, which are the focus of this study. Moreover, traditional
tools in statistical learning theory have not been able to provide a fully satisfying generalization
theory for neural networks.
Understanding the risk surface during neural network training is crucial for establishing a strong
theoretical foundation for neural network-based machine learning, particularly for understanding
generalization. Recent studies on neural networks suggest intriguing properties of the risk surface.
In large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,
global minima exist in each connected component of the risk’s sublevel set and are path-connected.
observed on the ’most challenging’ inputs that include at least one constituent that was not present in
the training data. However, clear indications of ’lexical memorization’ effects are evident in our error
analysis of unseen compounds.
Typically, the transfer of representations or sharing between tasks is more effective at the embedding
layers, which represent the model’s internal representation of the compound constituents. Furthermore,
in multi-task learning, the complete sharing of model architecture across tasks degrades its capacity
to generalize when it comes to less frequent relations.
The dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound
interpretation because it links this sub-problem with broad-coverage semantic role labeling or
