Generalization in ReLU Networks via Restricted
Isometry and Norm Concentration
Abstract
Regression tasks, while aiming to model relationships across the entire input space,
are often constrained by limited training data. Nevertheless, if the hypothesis func-
tions can be represented effectively by the data, there is potential for identifying a
model that generalizes well. This paper introduces the Neural Restricted Isometry
Property (NeuRIPs), which acts as a uniform concentration event that ensures all
shallow ReLU networks are sketched with comparable quality. To determine the
sample complexity necessary to achieve NeuRIPs, we bound the covering numbers
of the networks using the Sub-Gaussian metric and apply chaining techniques. As-
 A Rectified Linear Unit (ReLU) function φ:R→Ris given by φ(x) := max( x,0).
Given a weight vector w∈Rd, a bias b∈R, and a sign κ∈ {± 1}, a ReLU neuron is a function
φ(w, b, κ ) :Rd→Rdefined as
φ(w, b, κ )(x) =κφ(wTx+b).
Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented
by a graph with nneurons in a single hidden layer. When using the ReLU activation function, we can
apply a symmetry procedure to represent these as sums:
 While these traditional complexity
notions have been successful in classification problems, they do not apply to generic regression
problems with unbounded risk functions, which are the focus of this study. Moreover, traditional
tools in statistical learning theory have not been able to provide a fully satisfying generalization
theory for neural networks.
Understanding the risk surface during neural network training is crucial for establishing a strong
theoretical foundation for neural network-based machine learning, particularly for understanding
generalization. Recent studies on neural networks suggest intriguing properties of the risk surface.
In large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,
global minima exist in each connected component of the risk’s sublevel set and are path-connected.
