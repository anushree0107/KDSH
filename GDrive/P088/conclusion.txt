Generalization in ReLU Networks via Restricted
Isometry and Norm Concentration
Abstract
Regression tasks, while aiming to model relationships across the entire input space,
are often constrained by limited training data. Nevertheless, if the hypothesis func-
tions can be represented effectively by the data, there is potential for identifying a
model that generalizes well. This paper introduces the Neural Restricted Isometry
Property (NeuRIPs), which acts as a uniform concentration event that ensures all
shallow ReLU networks are sketched with comparable quality. To determine the
sample complexity necessary to achieve NeuRIPs, we bound the covering numbers
of the networks using the Sub-Gaussian metric and apply chaining techniques. As-
 A Rectified Linear Unit (ReLU) function φ:R→Ris given by φ(x) := max( x,0).
Given a weight vector w∈Rd, a bias b∈R, and a sign κ∈ {± 1}, a ReLU neuron is a function
φ(w, b, κ ) :Rd→Rdefined as
φ(w, b, κ )(x) =κφ(wTx+b).
Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented
by a graph with nneurons in a single hidden layer. When using the ReLU activation function, we can
apply a symmetry procedure to represent these as sums:
When used to produce proximity functions as given in Equation 1, these values help ensure safety.
Figure 5 shows examples of the unsafeable region, distance function, and proximity function for the
CL1500 advisory.
C.3 Structure of Predictors
The compressed versions of the policy tables from prior work are neural networks with six hidden
layers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecture
for our standard, unconstrained network. For constrained predictors, we use a similar architecture.
However, the first four hidden layers are shared between all of the predictors. This learns a single,
shared input space representation, and also allows each predictor to adapt to its constraints. Each
