This paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-
ties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or
detection, our dataset emphasizes fine-grained activity recognition. The differences between activities
are often minimal, primarily involving the movement of a single individual, with a consistent scene
structure across activities. The determination of activity is based on a single camera perspective. This
study compares various methods for temporal feature aggregation, both for classifying activities in
segmented videos and for detecting them in continuous video streams.
2 Related Work
The field of activity recognition has garnered substantial attention in computer vision research. Initial
successes were achieved with hand-engineered features such as dense trajectories.i=1, keyframes {IK
j}k
j=1⊆ {IDi}n
i=1are
chosen. A method is implemented to detect and remove duplicate and blurry images, ensuring
high-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier
transform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-
ing to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded
to maintain data integrity and accuracy.
Using the selected keyframes {IK
j}k
j=1, the team estimates camera poses through a method called
We have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available
on YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos
intended for activity recognition and continuous videos designed for activity classification. The
dataset’s complexity is amplified by the fact that it originates from televised baseball games, where a
single camera perspective is shared among various activities. Additionally, there is minimal variance
in motion and appearance among different activities, such as swinging a bat versus bunting. In
contrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities
with diverse settings, scales, and camera angles, our dataset features activities where a single frame
