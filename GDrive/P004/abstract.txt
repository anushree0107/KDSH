in the label sets, where it exists, can be leveraged through transfer and multi-task learning, especially
since the overall distribution of relations differs between the two frameworks.
4 Transfer vs. Multi-Task Learning
In this section, we employ the terminology and definitions established by Pan and Yang (2010) to
articulate our framework for transfer and multi-task learning. Our classification task can be described
in terms of all training pairs (X, Y) and a probability distribution P(X), where X represents the input
feature space, Y denotes the set of all labels, and N is the training data size. The domain of a task is
defined by X, P(X).Generalization in ReLU Networks via Restricted
Isometry and Norm Concentration
Abstract
Regression tasks, while aiming to model relationships across the entire input space,
are often constrained by limited training data. Nevertheless, if the hypothesis func-
tions can be represented effectively by the data, there is potential for identifying a
model that generalizes well. This paper introduces the Neural Restricted Isometry
Property (NeuRIPs), which acts as a uniform concentration event that ensures all
shallow ReLU networks are sketched with comparable quality. To determine the
sample complexity necessary to achieve NeuRIPs, we bound the covering numbers
of the networks using the Sub-Gaussian metric and apply chaining techniques. As-
note that this does not necessarily imply that we aim to use a single model to predict both label sets
in practice.
5 Neural Classification Models
This section introduces the neural classification models utilized in our experiments. To discern the
impact of TL and MTL, we initially present a single-task learning model, which acts as our baseline.
Subsequently, we employ this same model to implement TL and MTL.
5.1 Single-Task Learning Model
In our single-task learning (STL) configuration, we train and fine-tune a feed-forward neural network
inspired by the neural classifier proposed by Dima and Hinrichs (2015). This network comprises four
