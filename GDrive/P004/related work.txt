in the label sets, where it exists, can be leveraged through transfer and multi-task learning, especially
since the overall distribution of relations differs between the two frameworks.
4 Transfer vs. Multi-Task Learning
In this section, we employ the terminology and definitions established by Pan and Yang (2010) to
articulate our framework for transfer and multi-task learning. Our classification task can be described
in terms of all training pairs (X, Y) and a probability distribution P(X), where X represents the input
feature space, Y denotes the set of all labels, and N is the training data size. The domain of a task is
defined by X, P(X).5)1
δfort = 0,1, . . . , T −1
The filters are then generated as:
Fm[i, t] =1
Zmexp
−(t−μi,m)2
2σ2m
i∈ {0,1, . . . , N −1}, t∈ {0,1, . . . , T −1}
where Zmis a normalization constant.
We apply these filters Fto the T×Dvideo representation through matrix multiplication, yielding an
N×Drepresentation that serves as input to a fully-connected layer for classification. This method
is shown in Fig 5(d).
 The random sample x∈Rdand label y∈Rfollow a joint distribution μsuch that
the marginal distribution μxof sample x is standard Gaussian with density
1
(2π)d/2exp
−∥x∥2
2
.
As available data, we assume independent copies {(xj, yj)}m
j=1of the random pair (x, y), each
distributed by μ.
3 Concentration of the Empirical Norm
Supervised learning algorithms interpolate labels yfor samples x, both distributed jointly by μon
X × Y . This task is often solved under limited data accessibility. The training data, respecting
