The application of transfer and multi-task learning in natural language processing has gained sig-
nificant traction, yet considerable ambiguity persists regarding the effectiveness of particular task
characteristics and experimental setups. This research endeavors to clarify the benefits of TL and
MTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence of
minimally contrasting experiments and conducting thorough analysis of results and prediction errors,
we demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically
enhance predictions for low-frequency relations. Overall, our TL, and particularly our MTL models,
are better at making predictions both quantitatively and qualitatively. Notably, the improvements are
observed on the ’most challenging’ inputs that include at least one constituent that was not present in
the training data. However, clear indications of ’lexical memorization’ effects are evident in our error
analysis of unseen compounds.
Typically, the transfer of representations or sharing between tasks is more effective at the embedding
layers, which represent the model’s internal representation of the compound constituents. Furthermore,
in multi-task learning, the complete sharing of model architecture across tasks degrades its capacity
to generalize when it comes to less frequent relations.
The dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound
interpretation because it links this sub-problem with broad-coverage semantic role labeling or
Several recent studies have conducted extensive experiments on the application of TL and MTL to a
variety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentiment
classification, super-tagging, chunking, and semantic dependency parsing. The consensus among
these studies is that the advantages of TL and MTL are largely contingent on the characteristics of the
tasks involved, including the unevenness of the data distribution, the semantic relatedness between
the source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasks
that quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structural
similarity between the tasks.