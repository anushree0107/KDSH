Mbacke et al. (2023), and the proofs presented are fundamental.
1 Introduction
Diffusion models, alongside generative adversarial networks and variational autoencoders (V AEs), are among the most influential
families of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,
as well as in various other applications.
Two primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative
models (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while
simultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Consequently, the representation of the question is concatenated with the representation of
each forecast before being fed into the LSTM. Finally, the last hidden state of the LSTM is connected
to a fully connected layer with a single neuron and sigmoid activation to produce the final prediction
for the question.
4.3 Architecture Ablation
Experiments are carried out with the complete neural architecture, as described above, as well as
with variations where certain components are disabled. Specifically, the representation of a forecast
is manipulated by incorporating different combinations of information:
4
* Only the prediction. * The prediction and the representation of the question. * The prediction and
the representation of the justification. * The prediction, the representation of the question, and the
neural network configurations, except for the one using only predictions and justifications.
**Encoding Questions and Justifications** The neural network that only utilizes the prediction
to represent a forecast surpasses both baseline methods. Notably, integrating the question, the
justification, or both into the forecast representation yields further improvements. These results
indicate that incorporating the question and forecaster-provided justifications into the model enhances
the accuracy of question calling.
**Calling Questions Throughout Their Life** When examining the results across the four quartiles of
a question’s duration, it’s observed that while using active forecasts is beneficial across all quartiles
for both baselines and all network configurations, the neural networks surprisingly outperform the
