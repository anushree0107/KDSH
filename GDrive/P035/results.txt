 This ensures consistent reference object identification throughout the dataset.
To create RGBA images, the team combines RGB images, reference object masks {MR
i}n
i=1, and
food object masks {MF
i}n
i=1. This step, denoted as {IR
i}n
i=1, integrates various data sources into a
unified format for further processing.
The team converts the RGBA images {IR
i}n
i=1and camera poses {Cj}k
j=1into meaningful metadata
and modeled data Dm. This transformation facilitates accurate scene reconstruction.
The modeled data Dmis input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes
applicability.
•Complex backgrounds and objects: The method has not been tested in environments with
complex backgrounds or highly intricate food objects.
•Capturing complexities: The method has not been evaluated under different capturing
complexities, such as varying distances and camera speeds.
•Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.
They aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve
efficiency.
6
Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance
L Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m
1 40.06 38.technologies are vital for fostering healthier eating behaviors and addressing health issues linked to
diet.
By concentrating on the development of accurate 3D models of food derived from various visual
inputs, including multiple views and single perspectives, this challenge endeavors to bridge the
disparity between current methodologies and practical needs. It promotes the creation of unique
solutions capable of managing the intricacies of food morphology, texture, and illumination, while also
meeting the real-world demands of dietary evaluation. This initiative gathers experts from computer
vision, machine learning, and nutrition science to propel 3D food reconstruction technologies forward.
These advancements have the potential to substantially enhance the precision and utility of food
