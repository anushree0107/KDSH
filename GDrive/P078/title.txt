to utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However, in LSTM layers, the local
context is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time
might have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and
self-attention layers, which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1∈RN×d
and a secondary input ˆx2∈RN×dand yields:
 Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal
6
Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose
heatmaps.
Method Accuracy
Random 17.0%
I3D 25.8%
I3D + LSTM 18.5%
I3D + sub-events 34.5%
Pose 28.4%
Pose + LSTM 27.6%
Pose + sub-events 36.4%
convolution appear to overfit. Convolutional sub-events, especially when combined with super-event
representation, significantly enhance performance, particularly for frame-based features.
 Consequently, the representation of the question is concatenated with the representation of
each forecast before being fed into the LSTM. Finally, the last hidden state of the LSTM is connected
to a fully connected layer with a single neuron and sigmoid activation to produce the final prediction
for the question.
4.3 Architecture Ablation
Experiments are carried out with the complete neural architecture, as described above, as well as
with variations where certain components are disabled. Specifically, the representation of a forecast
is manipulated by incorporating different combinations of information:
4
* Only the prediction. * The prediction and the representation of the question. * The prediction and
the representation of the justification. * The prediction, the representation of the question, and the
