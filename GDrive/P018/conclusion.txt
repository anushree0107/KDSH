Safe Predictors for Input-Output Specification
Enforcement
Abstract
This paper presents an approach for designing neural networks, along with other
machine learning models, which adhere to a collection of input-output specifica-
tions. Our method involves the construction of a constrained predictor for each set
of compatible constraints, and combining these predictors in a safe manner using a
convex combination of their predictions. We demonstrate the applicability of this
method with synthetic datasets and on an aircraft collision avoidance problem.
1 Introduction
The increasing adoption of machine learning models, such as neural networks, in safety-critical
applications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgent
need for the development of guarantees on safety and robustness.• a standard neural network architecture gb:X→Rm,
and then defining Gb(x;θb) =hb(gb(x;θb)).
The framework proposed here does not require an entirely separate network for each b. In many
applications, it may be advantageous for the constrained predictors to share earlier layers, thus
creating a shared representation of the input space. In addition, our definition of the safe predictor is
general and is not limited to neural networks.
In Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D
with simple neural networks. These examples show that our safe predictor can enforce arbitrary
input-output specifications using convex output constraints on neural networks, and that the learned
When used to produce proximity functions as given in Equation 1, these values help ensure safety.
Figure 5 shows examples of the unsafeable region, distance function, and proximity function for the
CL1500 advisory.
C.3 Structure of Predictors
The compressed versions of the policy tables from prior work are neural networks with six hidden
layers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecture
for our standard, unconstrained network. For constrained predictors, we use a similar architecture.
However, the first four hidden layers are shared between all of the predictors. This learns a single,
shared input space representation, and also allows each predictor to adapt to its constraints. Each
