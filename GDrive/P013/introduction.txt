 The "majority vote" baseline determines the answer to a
question based on the most frequent prediction among the forecasts. The "weighted vote" baseline,
on the other hand, assigns weights to the probabilities in the predictions and then aggregates them.
4.2 Neural Network Architecture
A neural network architecture is employed, which consists of three main components: one to generate
a representation of the question, another to generate a representation of each forecast, and an LSTM
to process the sequence of forecasts and ultimately call the question.
The representation of a question is obtained using BERT, followed by a fully connected layer with 256
neurons, ReLU activation, and dropout. The representation of a forecast is created by concatenating
 A Rectified Linear Unit (ReLU) function φ:R→Ris given by φ(x) := max( x,0).
Given a weight vector w∈Rd, a bias b∈R, and a sign κ∈ {± 1}, a ReLU neuron is a function
φ(w, b, κ ) :Rd→Rdefined as
φ(w, b, κ )(x) =κφ(wTx+b).
Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented
by a graph with nneurons in a single hidden layer. When using the ReLU activation function, we can
apply a symmetry procedure to represent these as sums:
• a standard neural network architecture gb:X→Rm,
and then defining Gb(x;θb) =hb(gb(x;θb)).
The framework proposed here does not require an entirely separate network for each b. In many
applications, it may be advantageous for the constrained predictors to share earlier layers, thus
creating a shared representation of the input space. In addition, our definition of the safe predictor is
general and is not limited to neural networks.
In Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D
with simple neural networks. These examples show that our safe predictor can enforce arbitrary
input-output specifications using convex output constraints on neural networks, and that the learned
