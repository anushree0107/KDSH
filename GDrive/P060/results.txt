L/2,L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,
and the pooled features are concatenated, yielding a 14×D-dimensional representation for each
window, which is then used as input to the classifier.
For temporal convolutional models in continuous videos, we modify the segmented video approach by
learning a temporal convolutional kernel of length Land convolving it with the input video features.
This operation transforms input of size T×Dinto output of size T×D, followed by a per-frame
classifier. This enables the model to aggregate local temporal information.
We have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available
on YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos
intended for activity recognition and continuous videos designed for activity classification. The
dataset’s complexity is amplified by the fact that it originates from televised baseball games, where a
single camera perspective is shared among various activities. Additionally, there is minimal variance
in motion and appearance among different activities, such as swinging a bat versus bunting. In
contrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities
with diverse settings, scales, and camera angles, our dataset features activities where a single frame
applicability.
•Complex backgrounds and objects: The method has not been tested in environments with
complex backgrounds or highly intricate food objects.
•Capturing complexities: The method has not been evaluated under different capturing
complexities, such as varying distances and camera speeds.
•Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.
They aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve
efficiency.
6
Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance
L Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m
1 40.06 38.