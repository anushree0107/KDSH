of complexity: easy (200 images), medium (30 images), and hard (1 image). A
total of 16 teams participated in the final assessment phase. The methodologies
developed during this challenge have yielded highly encouraging outcomes in
3D food reconstruction, showing great promise for refining portion estimation in
dietary evaluations and nutritional tracking. Further information on this workshop
challenge and the dataset is accessible via the provided URL.
1 Introduction
The convergence of computer vision technologies with culinary practices has pioneered innovative
approaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challenge
represents a landmark initiative in this emerging field, responding to the pressing demand for precise
and scalable techniques for estimating food portions and monitoring nutritional consumption. Such
Advancements in 3D Food Modeling: A Review of the
MetaFood Challenge Techniques and Outcomes
Abstract
The growing focus on leveraging computer vision for dietary oversight and nutri-
tion tracking has spurred the creation of sophisticated 3D reconstruction methods
for food. The lack of comprehensive, high-fidelity data, coupled with limited
collaborative efforts between academic and industrial sectors, has significantly
hindered advancements in this domain. This study addresses these obstacles by
introducing the MetaFood Challenge, aimed at generating precise, volumetrically
accurate 3D food models from 2D images, utilizing a checkerboard for size cal-
ibration. The challenge was structured around 20 food items across three levels
portion estimation across diverse applications, from individual health tracking to extensive nutritional
investigations.
Conventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires
(FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can be
burdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-
based methods for estimating food portions directly from images of eating occasions. By enhancing
3D reconstruction for food, the aim is to provide more accurate and intuitive nutritional assessment
tools. This technology could revolutionize the sharing of culinary experiences and significantly
impact nutrition science and public health.
