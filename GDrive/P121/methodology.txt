 x∈
Aunsafeable ,i⇒Fi(x)<max jFj(x), where Fj(x)is the output score for the jthadvisory.
Table 1 shows the performance of a standard, unconstrained network and our safe predictor. For both
networks, we present the percentage accuracy (ACC) and violations (percentage of inputs for which
the network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,
based on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).
As shown in the table, our safe predictor adheres to the required safeability property. Furthermore,
 We prove this property in a network
recovery setting in Theorem 2, and also an agnostic learning setting in Theorem 3. These results
ensure a small generalization error, when any optimization algorithm finds a network with a small
empirical risk. We develop the key proof techniques for deriving the sample complexity of achieving
NeuRIPs in Section V , by using the chaining theory of stochastic processes. The derived results are
summarized in Section VI, where we also explore potential future research directions.
2 Notation and Assumptions
In this section, we will define the key notations and assumptions for the neural networks examined
in this study.suming the NeuRIPs event, we then provide bounds on the expected risk, applicable
to networks within any sublevel set of the empirical risk. Our results show that all
networks with sufficiently small empirical risk achieve uniform generalization.
1 Introduction
A fundamental requirement of any scientific model is a clear evaluation of its limitations. In recent
years, supervised machine learning has seen the development of tools for automated model discovery
from training data. However, these methods often lack a robust theoretical framework to estimate
model limitations. Statistical learning theory quantifies the limitation of a trained model by the
generalization error. This theory uses concepts such as the VC-dimension and Rademacher complexity
to analyze generalization error bounds for classification problems.