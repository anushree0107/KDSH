 It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,
termed "weak Minty," without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.
Convergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as
the update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the
length of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is
Table 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce
generalization error in NomBank across all scenarios, with the exception of TLH and TLEH for
completely unseen compounds, where error increases. The greatest error reductions are achieved
by MTL models across all three types of unseen compounds. Specifically, MTLE reduces the error
by approximately six points for compounds with unseen right constituents and by eleven points for
fully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituent
is unseen. Itâ€™s important to interpret these results in conjunction with the Count row in Table 9 for
a comprehensive view.but the label sets are distinct.
For clarity, we differentiate between transfer learning and multi-task learning in this paper, despite
these terms sometimes being used interchangeably in the literature. We define TL as the utilization of
parameters from a model trained on Ta to initialize another model for Tb. In contrast, MTL involves
training parts of the same model to learn both Ta and Tb, essentially learning one set of parameters
for both tasks. The concept is to train a single model simultaneously on both tasks, where one task
introduces an inductive bias that aids the model in generalizing over the main task. It is important to
