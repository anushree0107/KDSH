are updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)
optimization function across all models, with a learning rate set to 0.001. The loss function employed
is the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer.
All models are trained with mini-batches of size five. The maximum number of epochs is capped
at 50, but an early stopping criterion based on the modelâ€™s accuracy on the validation split is also
implemented. This means that training is halted if the validation accuracy does not improve over five
consecutive epochs. All models are implemented in Keras, using TensorFlow as the backend. The TL
sequential processing of video features, whereas other methods can be fully parallelized.
Table 3: Additional parameters required for models when added to the base model (e.g., I3D or
Inception V3).
Model # Parameters
Max/Mean Pooling 16K
Pyramid Pooling 115K
LSTM 10.5M
Temporal Conv 31.5M
Sub-events 36K
Table 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.
Learning sub-intervals for pooling is found to be crucial for activity recognition.
Method RGB Flow Two-stream
Random 16.3 16.3 16.3
 We also experiment with combining the super- and sub-event representations
to form a three-level hierarchy for event representation.
6 Experiments
6.1 Implementation Details
For our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics
datasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable
feature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet
and Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth
compared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow
