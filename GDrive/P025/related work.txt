PixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, and
refining them. The outputs are the camera poses {Cj}k
j=1, crucial for understanding the scene’s
spatial layout.
4
In parallel, the team uses a tool called SAM for reference object segmentation. SAM segments
the reference object with a user-provided prompt, producing a reference object mask MRfor each
keyframe. This mask helps track the reference object across all frames. The XMem++ method
extends the reference object mask MRto all frames, creating a comprehensive set of reference object
masks {MR
i}n
i=1. We also experiment with combining the super- and sub-event representations
to form a three-level hierarchy for event representation.
6 Experiments
6.1 Implementation Details
For our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics
datasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable
feature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet
and Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth
compared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow
Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state
as input to a fully-connected layer for classification. We frame our tasks as multi-label classification
and train these models to minimize binary cross-entropy:
L(v) =X
czclog(p(c|G(v))) + (1 −zc) log(1 −p(c|G(v)))
where G(v)is the function that pools the temporal information, and zcis the ground truth label for
class c.
5 Activity Detection in Continuous Videos
Detecting activities in continuous videos poses a greater challenge. The goal here is to classify each
frame according to the activities occurring. Unlike segmented videos, continuous videos feature
