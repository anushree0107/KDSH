Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state
as input to a fully-connected layer for classification. We frame our tasks as multi-label classification
and train these models to minimize binary cross-entropy:
L(v) =X
czclog(p(c|G(v))) + (1 −zc) log(1 −p(c|G(v)))
where G(v)is the function that pools the temporal information, and zcis the ground truth label for
class c.
5 Activity Detection in Continuous Videos
Detecting activities in continuous videos poses a greater challenge. The goal here is to classify each
frame according to the activities occurring. Unlike segmented videos, continuous videos feature
 LSTMs performed worse than the
baseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the
easiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult
(12%).
6.3 Continuous Video Activity Detection
We evaluate models extended for continuous videos using per-frame mean average precision (mAP),
with results shown in Table 8. This setting is more challenging than segmented videos, requiring
the model to identify activity start and end times and handle ambiguous negative examples. All
models improve upon the baseline per-frame classification, confirming the importance of temporal
information.which capture red-green-blue (RGB) and depth data 2-3 hours daily (during daylight hours at times when participants were at home).
The videos were then manually annotated to the nearest millisecond to provide localization labels. Multiple human labelers used
software called ELAN to watch up to 4 simultaneously-captured video files at a time. The resulting labeled data recorded the kitchen,
hallway, dining room, living room, stairs, and porch. The duration of labeled data recorded by the cameras for PD and HC is 72.84
and 75.31 hours, respectively, which provides a relatively balanced label set for our room-level classification. Finally, to evaluate
