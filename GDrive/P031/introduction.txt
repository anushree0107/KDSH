our attention on methods that frame the interpretation problem as a classification task involving a
fixed, predetermined set of relations. Various machine learning models have been applied to this
task, including nearest neighbor classifiers that use semantic similarity based on lexical resources,
kernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy
models that incorporate a wide range of lexical and surface form features, and neural networks that
rely on word embeddings or combine word embeddings with path embeddings. Among these studies,
some have utilized the same dataset. To our knowledge, TL and MTL have not been previously
applied to compound interpretation. Therefore, we review prior research on TL and MTL in other
NLP tasks.
In computational linguistics, noun-noun compound interpretation is typically treated as an automatic
classification task. Various machine learning (ML) algorithms and models, such as Maximum
Entropy, Support Vector Machines, and Neural Networks, have been employed to decipher the
semantics of nominal compounds. These models utilize information from lexical semantics, like
WordNet-based features, and distributional semantics, such as word embeddings. However, noun-
noun compound interpretation remains a challenging NLP problem due to the high productivity
of noun-noun compounding as a linguistic structure and the difficulty in deriving the semantics of
noun-noun compounds from their constituents. Our research contributes to advancing NLP research
Several recent studies have conducted extensive experiments on the application of TL and MTL to a
variety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentiment
classification, super-tagging, chunking, and semantic dependency parsing. The consensus among
these studies is that the advantages of TL and MTL are largely contingent on the characteristics of the
tasks involved, including the unevenness of the data distribution, the semantic relatedness between
the source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasks
that quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structural
similarity between the tasks.