layers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input
3
layer consists of two integers that indicate the indices of a compound’s constituents in the embedding
layer, where the word embedding vectors are stored. These selected vectors are then passed to a fully
connected hidden layer, the size of which matches the dimensionality of the word embedding vectors.
Finally, a softmax function is applied to the output layer to select the most probable relation.
The compound’s constituents are represented using a 300-dimensional word embedding model trained
on an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model was
our attention on methods that frame the interpretation problem as a classification task involving a
fixed, predetermined set of relations. Various machine learning models have been applied to this
task, including nearest neighbor classifiers that use semantic similarity based on lexical resources,
kernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy
models that incorporate a wide range of lexical and surface form features, and neural networks that
rely on word embeddings or combine word embeddings with path embeddings. Among these studies,
some have utilized the same dataset. To our knowledge, TL and MTL have not been previously
applied to compound interpretation. Therefore, we review prior research on TL and MTL in other
NLP tasks.
A key reason for utilizing multi-task learning is to enhance generalization by making use of the
domain-specific details present in the training data of related tasks. In this study, we demonstrate that
TL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations
within a dataset marked by a highly skewed distribution of relations. This dataset is particularly
well-suited for TL and MTL experimentation, as elaborated in Section 3.
Our contributions are summarized as follows:
1. Through meticulous analysis of results, we discover that TL and MTL, especially when applied
to the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a
