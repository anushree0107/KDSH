 For overall method performance, Table 4 shows
the MAPE and Chamfer distance with and without transformation metrics.
Additionally, qualitative results on one- and few-shot 3D reconstruction from the challenge dataset
are shown. The model excels in texture details, artifact correction, missing data handling, and color
adjustment across different scene parts.
Limitations: Despite promising results, several limitations need to be addressed in future work:
•Manual processes: The current pipeline includes manual steps like providing segmentation
prompts and identifying scaling factors, which should be automated to enhance efficiency.
•Input requirements: The method requires extensive input information, including food
masks and depth data. Streamlining these inputs would simplify the process and increase
Hamming distance for near image similarity was set to 12. For Gaussian kernel radius, even numbers
in the range [0...30] were used for detecting blurry images. The diameter for removing isolated pieces
was set to 5%. NeuS2 was run for 15,000 iterations with a mesh resolution of 512x512, a unit cube
"aabb scale" of 1, "scale" of 0.15, and "offset" of [0.5, 0.5, 0.5] for each food scene.
5
4.2.2 VolETA Results
The team extensively validated their approach on the challenge dataset and compared their results
 We also experiment with combining the super- and sub-event representations
to form a three-level hierarchy for event representation.
6 Experiments
6.1 Implementation Details
For our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics
datasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable
feature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet
and Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth
compared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow
