different positive and negative samples to mitigate representation separation from a popularity-centric perspective. We incorporate
this approach into contrastive learning to better optimize the consistency of representations. Specifically, we aim to reduce the risk
of pushing items with varying popularity further apart. For example, when using a popular item as a positive sample, our goal is
to avoid pushing unpopular items too far away. Thus, we introduce two hyperparameters to control the weights when items are
considered positive and negative samples.
To ensure balanced and equitable representations of items within our model, we first propose a dynamic strategy to categorize items
into popular and unpopular groups for each mini-batch. Instead of relying on a fixed global threshold, which often leads to the
 The "majority vote" baseline determines the answer to a
question based on the most frequent prediction among the forecasts. The "weighted vote" baseline,
on the other hand, assigns weights to the probabilities in the predictions and then aggregates them.
4.2 Neural Network Architecture
A neural network architecture is employed, which consists of three main components: one to generate
a representation of the question, another to generate a representation of each forecast, and an LSTM
to process the sequence of forecasts and ultimately call the question.
The representation of a question is obtained using BERT, followed by a fully connected layer with 256
neurons, ReLU activation, and dropout. The representation of a forecast is created by concatenating
unpopular items. α-AdjNorm enhances the focus on unpopular items by controlling the normalization strength during the
neighborhood aggregation process in GCN-based models.
•Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)
and popularity. For instance, MACR uses counterfactual reasoning to eliminate the direct impact of popularity on item
outcomes. In contrast, InvCF operates on the principle that item representations remain invariant to changes in popularity
semantics, filtering out unstable or outdated popularity characteristics to learn unbiased representations.
•Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE, preserving
more inherent characteristics of items to mitigate popularity bias.