layers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input
3
layer consists of two integers that indicate the indices of a compound’s constituents in the embedding
layer, where the word embedding vectors are stored. These selected vectors are then passed to a fully
connected hidden layer, the size of which matches the dimensionality of the word embedding vectors.
Finally, a softmax function is applied to the output layer to select the most probable relation.
The compound’s constituents are represented using a 300-dimensional word embedding model trained
on an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model was
Generalization in ReLU Networks via Restricted
Isometry and Norm Concentration
Abstract
Regression tasks, while aiming to model relationships across the entire input space,
are often constrained by limited training data. Nevertheless, if the hypothesis func-
tions can be represented effectively by the data, there is potential for identifying a
model that generalizes well. This paper introduces the Neural Restricted Isometry
Property (NeuRIPs), which acts as a uniform concentration event that ensures all
shallow ReLU networks are sketched with comparable quality. To determine the
sample complexity necessary to achieve NeuRIPs, we bound the covering numbers
of the networks using the Sub-Gaussian metric and apply chaining techniques. As-
 The training uses a sampled subset of points from the input space and the learned
predictors are shown for the continuous input space.
Figure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D
input and 1-D output and two overlapping constraints. The unconstrained network has two hidden
layers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained
predictors G00,G10,G01andG11share the hidden layers and have an additional hidden layer of size
20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points
from the input space and the learned predictors are shown for the continuous input space.
