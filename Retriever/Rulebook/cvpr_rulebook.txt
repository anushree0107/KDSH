● R006
Review of "Detailed Action Identification in Baseball Game Recordings"
The paper introduces MLB-YouTube, a new dataset tailored for fine-grained activity recognition
in baseball videos, focusing on segmented and continuous video tasks. The authors evaluate
various temporal feature aggregation methods, emphasizing sub-events and super-events to
enhance recognition and detection performance.
Strengths:
1. Novel Dataset: MLB-YouTube fills a gap in sports analytics by providing densely
annotated baseball activities with nuanced distinctions.
2. Comprehensive Methods: A thorough comparison of temporal pooling, LSTMs, and
convolutional approaches enriches understanding of their strengths.
3. Practical Relevance: Tasks like pitch speed regression and pitch type classification
demonstrate the dataset's utility in real-world scenarios.
Weaknesses:
1. Limited Generalizability: The dataset focuses exclusively on baseball, which may
restrict broader applications.
2. Overfitting Concerns: High-parameter models like LSTMs and temporal convolutions
suffer from overfitting.
3. Modest Accuracy for Pitch Type Classification: While sub-events improve results,
overall performance remains low for certain pitch types.
Suggestions:
● Extend the dataset's scope to other sports for wider applicability.
● Optimize models to reduce overfitting.
● Enhance visualizations of temporal hierarchies for clarity.
Minor Issues:
● Include confidence intervals for results to ensure robustness.
● Elaborate on how frame rates influence pitch speed prediction accuracy.
Overall:
This paper provides a valuable contribution to sports video analysis, but addressing limitations
in generalization, model optimization, and presentation could further enhance its impact.

● R007
Review of "Advancements in 3D Food Modeling: A Review of the MetaFood Challenge
Techniques and Outcomes"
Strengths:
1. Novel Dataset and Benchmark: The MetaFood Challenge introduces a valuable
dataset and evaluation pipeline, specifically designed for 3D food reconstruction,
addressing critical gaps in dietary monitoring and nutrition tracking.
2. Comprehensive Methodologies: The paper provides an in-depth review of innovative
techniques employed by top-performing teams, including multi-view and single-view 3D
reconstruction, sub-event scaling, and advanced mesh refinement.
3. Practical Relevance: By focusing on volumetric accuracy and realistic 3D modeling of
food items, the challenge offers significant implications for dietary assessment and
broader health applications.
Weaknesses:
1. Dataset Scope: While comprehensive, the dataset is limited to 20 food items, which
might not capture the diversity of real-world dietary scenarios.
2. Dependence on Manual Inputs: Several approaches relied on manual scaling and
segmentation, limiting scalability and automation potential.
3. Limited Testing Environments: The methods were not evaluated under complex
conditions, such as diverse lighting, backgrounds, or camera variations, which could
impact real-world applicability.
Suggestions for Improvement:
1. Expand the dataset to include more diverse food items and complex scenarios to
improve generalizability.
2. Automate manual steps like scaling factor determination and segmentation to enhance
usability.
3. Test methods in more challenging environments to evaluate robustness.
Minor Issues:
● Include more detailed visualizations of reconstructed models to enhance clarity.
● Provide quantitative analyses of computational efficiency across different methods.
Overall:
The paper presents a significant step forward in 3D food modeling, showcasing innovative
solutions and setting a strong foundation for future research. Addressing limitations related to
dataset diversity, automation, and robustness will further solidify its impact in the field.

Review Summary (Unsupervised Template-assisted Point Cloud Shape Correspondence Network)
Summary of ContribuƟons: The paper introduces TANet, a novel unsupervised template-assisted
network for point cloud shape correspondence. It includes a template generaƟon module for
creaƟng learnable templates and a template assistance module to enhance correspondence

accuracy. TANet demonstrates its effecƟveness on benchmarks like TOSCA, SHREC'19, and cross-
dataset evaluaƟons with SMAL and SURREAL, outperforming state-of-the-art methods and showing

robustness to noisy and incomplete data.

Strengths:
1. Novel Approach: The introducƟon of a template generaƟon and assistance module is a
unique contribuƟon to unsupervised point cloud correspondence.
2. Comprehensive Experiments: Extensive evaluaƟons across mulƟple datasets and scenarios,
including robustness analysis and cross-dataset generalizaƟon.
3. PracƟcal ImplicaƟons: The method effecƟvely handles challenging cases like unconvenƟonal
and noisy shapes, making it suitable for real-world applicaƟons.

Weaknesses:
1. ComputaƟonal Overhead: The transiƟve similarity computaƟon introduces increased
complexity, as acknowledged in the ablaƟon studies.
2. Template OpƟmality: While learnable templates provide structural guidance, the paper
admits they remain sub-opƟmal for highly complex or diverse shapes.
3. EvaluaƟon Depth: Limited discussion on the trade-offs of using fewer templates or direct
comparisons to mesh-based methods under similar constraints.

RaƟng and Confidence:
 RaƟng: 7/10 (Accept)
 Confidence: 4/5 (High Confidence)

Review Summary (X-3D: Explicit 3D Structure Modeling for Point Cloud RecogniƟon)
Summary of ContribuƟons:
The paper introduces X-3D, a novel explicit 3D structure modeling framework for point cloud
recogniƟon. It captures explicit local structures in 3D space and uses them to generate shared
dynamic kernels for neighborhood points, reducing the gap between the embedding space and input
space. The method is versaƟle and improves performance across tasks like segmentaƟon,
classificaƟon, and detecƟon with minimal computaƟonal overhead. Results on benchmarks like
S3DIS, ScanObjectNN, and ScanNet show state-of-the-art performance.

Strengths:
1. Novel Paradigm: The explicit 3D structure modeling approach is a significant departure from
implicit methods, offering a beƩer geometric prior for point cloud data.
2. Broad Applicability: X-3D enhances various exisƟng models and demonstrates improvements
across mulƟple tasks, including segmentaƟon, classificaƟon, and detecƟon.
3. Efficiency: The proposed method achieves state-of-the-art performance with only modest
increases in computaƟonal cost and parameter size.
4. Robustness: The framework's explicit structure modeling makes it resilient to
transformaƟons like scaling and rotaƟon.

Weaknesses:
1. Complexity Trade-offs: While computaƟonal overhead is reduced compared to transformers,
the method sƟll adds some complexity, which may not suit resource-constrained seƫngs.
2. Limited Discussion on Failure Cases: The paper could explore in more depth scenarios where
X-3D underperforms, such as datasets with highly noisy or irregular structures.
3. Explicit Structure GeneralizaƟon: Although X-3D demonstrates strong results, its explicit
structure modeling may not generalize equally well to non-manifold or highly irregular data.

RaƟng and Confidence:
 RaƟng: 8/10 (Strong Accept)
 Confidence: 4/5 (High Confidence)

Review Summary (Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching)
Summary of ContribuƟons:

The paper introduces ICGNet, a novel stereo matching framework integraƟng intra-view and cross-
view geometric knowledge. It leverages pre-trained interest point detectors and matchers to

enhance disparity esƟmaƟon accuracy while maintaining computaƟonal efficiency during inference.
The framework demonstrates state-of-the-art performance on SceneFlow, KITTI 2015, and KITTI 2012
datasets and shows superior cross-domain generalizaƟon and robustness to occlusions.

Strengths:
1. Novel Dual-constraint Approach: The integraƟon of intra-view and cross-view geometric
constraints addresses limitaƟons in exisƟng stereo matching networks and improves their
interpretability.
2. State-of-the-art Performance: Comprehensive experiments showcase ICGNet's superiority
across syntheƟc and real-world datasets, with improved accuracy in challenging condiƟons
like reflecƟve surfaces and occlusions.
3. Efficiency: The proposed method avoids inference-Ɵme computaƟonal overhead, making it
suitable for real-Ɵme applicaƟons.
4. Robustness and GeneralizaƟon: The framework enhances domain generalizaƟon and
performs well on unseen datasets, which is a criƟcal advantage for pracƟcal deployment.

Weaknesses:
1. Limited Baseline Diversity: While the method demonstrates improvements, comparisons
with a broader range of models, including non-deep learning approaches, could strengthen
its case.
2. Complexity of Components: The ablaƟon studies highlight the dependency on
hyperparameter tuning and architecture choices, which might affect ease of replicaƟon.
3. Scope for Extended ApplicaƟons: The method's applicability beyond disparity esƟmaƟon,
such as 3D reconstrucƟon or mulƟ-view stereo, is not discussed.

RaƟng and Confidence:
 RaƟng: 8/10 (Strong Accept)
 Confidence: 4/5 (High Confidence)

Review Summary (APSeg: Auto-Prompt Network for Cross-Domain Few-Shot SemanƟc
SegmentaƟon)
Summary of ContribuƟons
The paper introduces APSeg, an innovaƟve framework that adapts the Segment Anything Model
(SAM) for Cross-Domain Few-Shot SemanƟc SegmentaƟon (CD-FSS). It addresses the challenges
posed by domain shiŌs and manual prompƟng requirements. The key contribuƟons include:
1. Dual Prototype Anchor TransformaƟon (DPAT): Fuses pseudo query prototypes and support
prototypes to create a domain-agnosƟc feature space.
2. Meta Prompt Generator (MPG): Automates the generaƟon of prompt embeddings using
meta-learning, bypassing manual visual prompƟng.
3. EvaluaƟon: APSeg shows significant performance improvements on CD-FSS benchmarks,
achieving superior results in 1-shot and 5-shot tasks across diverse datasets.
Strengths

1. Novel Methodology: Introduces DPAT and MPG to enhance SAM's generalizaƟon in cross-
domain scenarios.

2. Comprehensive EvaluaƟon: Demonstrates effecƟveness across four disƟnct datasets,
outperforming state-of-the-art models in both 1-shot and 5-shot seƫngs.

3. PracƟcal ImplementaƟon: The framework achieves automaƟon without requiring fine-
tuning or manual intervenƟon, streamlining deployment in varied domains.

Weaknesses
1. Baseline Clarity: Some baseline comparisons, especially with newer SAM adaptaƟons, are
underexplored. Further detailed comparisons with compeƟng manual and automaƟc
prompƟng methods could provide more context.
2. Generality of MPG: The robustness of the MPG module across more varied datasets (e.g.,
beyond the four evaluated) is not extensively discussed.
3. Parameter SensiƟvity: While ablaƟons are provided, more insights into parameter sensiƟvity
(e.g., output feature channels, embedding numbers) would strengthen reproducibility.
RaƟng and Confidence
 RaƟng: 7/10 (Strong Accept with Minor Revisions)
 Confidence: 4/5 (High Confidence)

Review Summary (CLIPtone: Unsupervised Learning for Text-Based Image Tone Adjustment)
Summary of ContribuƟons
This paper introduces CLIPtone, an innovaƟve framework for unsupervised text-based image tone
adjustment. Leveraging the CLIP model, the authors propose a novel hyper-network-based approach
that adapƟvely modulates a pre-trained image enhancement backbone. Key contribuƟons include:
1. Unsupervised Learning: Eliminates the need for paired datasets by uƟlizing CLIP's perceptual
criteria for alignment between text descripƟons and tonal adjustments.
2. InnovaƟve Architecture: Extends 3D LUT-based image enhancement models for adapƟve
tone adjustment via text descripƟons, enabling zero-shot predicƟon for novel descripƟons.
3. Extensive EvaluaƟon: Demonstrates superiority over state-of-the-art methods in qualitaƟve
and quanƟtaƟve metrics, including a user study.
Strengths
1. Unsupervised Framework: Reduces data collecƟon costs while supporƟng a wide range of
adjustments, including novel descripƟons unseen during training.
2. QuanƟtaƟve and QualitaƟve Excellence: Outperforms compeƟtors like T2ONet, CLIPstyler,
and IP2P in structure preservaƟon, alignment with text descripƟons, and aestheƟc quality.
3. PracƟcality and Efficiency: Achieves faster inference Ɵmes compared to most baselines,
while maintaining high-quality outputs.
Weaknesses
1. Local Adjustments: Limited to global tonal adjustments, lacking the ability to handle
localized modificaƟons.
2. CLIP Dependency: Inherits biases from the pre-trained CLIP model, leading to unintended
color biases in some cases.
3. ComparaƟve Scope: While the comparisons are robust, the exclusion of certain emerging
methods or variaƟons of CLIP-based models could limit the comprehensiveness.
RaƟng and Confidence
 RaƟng: 8/10 (Strong Accept with Minor Revisions)
 Confidence: 5/5 (Very High Confidence)

Review Summary (GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel
Objects)
Summary of ContribuƟons
The paper presents GenFlow, a novel framework for iteraƟve 6D pose refinement of unseen objects
using RGB and RGB-D inputs. Key contribuƟons include:
1. OpƟcal Flow-Based Refinement: Introduces a recurrent shape-constrained opƟcal flow
module, guided by 3D object geometry, to iteraƟvely refine 6D poses.

2. Cascade Network Architecture: Exploits mulƟ-scale correlaƟon volumes with a coarse-to-
fine refinement strategy for beƩer accuracy and robustness.

3. Benchmark Performance: Achieves state-of-the-art results on unseen object benchmarks in
the BOP challenge datasets using RGB and RGB-D inputs, outperforming compeƟng methods.
Strengths
1. InnovaƟve Methodology: Combines differenƟable PnP, projecƟve geometry, and opƟcal flow
for pose refinement, improving both accuracy and generalizaƟon to unseen objects.
2. Robust Performance: Outperforms baselines like MegaPose and OSOP in unseen object pose
esƟmaƟon while maintaining compeƟƟve results on seen object datasets.
3. Scalable and Efficient: The proposed GMM-based sampling strategy reduces the
computaƟonal cost of pose hypothesis generaƟon, enabling efficient coarse pose esƟmaƟon.
4. Comprehensive EvaluaƟon: Includes thorough benchmarks, ablaƟon studies, and
comparisons with state-of-the-art methods across diverse datasets.
Weaknesses
1. Inference Speed: GenFlow has a higher computaƟonal cost per iteraƟon compared to
MegaPose due to its iteraƟve updates, which could limit real-Ɵme applicaƟons.
2. Limited GeneralizaƟon Discussion: The paper could expand on performance in real-world
scenarios with extreme occlusion or cluƩered environments.
3. MulƟ-Hypothesis Dependency: While effecƟve, the mulƟ-hypothesis refinement strategy
increases inference Ɵme, and its scalability to large object sets needs further exploraƟon.
RaƟng and Confidence
 RaƟng: 8/10 (Strong Accept)
 Confidence: 5/5 (Very High Confidence)

Review Summary: Infinigen Indoors
Summary of ContribuƟons:
The paper presents Infinigen Indoors, a procedural generator for photorealisƟc indoor scenes,
building on the Infinigen system. It introduces new capabiliƟes: a library of procedural indoor assets,
a constraint-based arrangement system using a domain-specific language (DSL), and an export tool
for real-Ɵme simulators like Omniverse and Unreal Engine. The system enables diverse and
customizable scene generaƟon, demonstrated through applicaƟons such as shadow removal and
occlusion boundary detecƟon.

Strengths:
1. InnovaƟve Procedural GeneraƟon: Extends fully procedural generaƟon to indoor scenes,
enhancing customizaƟon and diversity.

2. Constraint-based Arrangement System: Introduces a DSL and solver for flexible, user-
specified constraints, a significant advancement for scene composiƟon.

3. Seamless IntegraƟon with Simulators: Provides export capabiliƟes for generated scenes to
real-Ɵme simulaƟon plaƞorms, bridging syntheƟc data generaƟon and pracƟcal applicaƟons.
4. Open Source: Ensures accessibility and encourages community contribuƟons.

Weaknesses:
1. ImplementaƟon Details: Some technical details, especially around the opƟmizaƟon
processes (e.g., solver moves and constraints), could benefit from addiƟonal clarity.
2. Baseline Comparison: The paper lacks quanƟtaƟve comparisons with leading syntheƟc
indoor dataset generators, such as SceneNet or InteriorNet, on downstream tasks.
3. Limited EvaluaƟon Diversity: While the presented tasks validate uƟlity, more varied
downstream applicaƟons (e.g., embodied AI tasks) could strengthen the claim of
generalizability.
4. Export Generality: The export process is dependent on Blender, potenƟally limiƟng usability
across other 3D modeling tools.

RaƟng and Confidence:
 RaƟng: 7/10 (Strong Accept with Improvements)
 Confidence: 4/5 (High Confidence)